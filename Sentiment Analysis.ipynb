{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19f9967a-e394-4cb0-af44-0f24094b6083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package shakespeare to\n",
      "[nltk_data]     /Users/sabinbasnet/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/shakespeare.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download() #NLTK will display a download manager showing all available and installed resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "027fabcb-8e26-459d-9795-0b36fe2cdea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     /Users/sabinbasnet/nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sabinbasnet/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package state_union to\n",
      "[nltk_data]     /Users/sabinbasnet/nltk_data...\n",
      "[nltk_data]   Package state_union is already up-to-date!\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /Users/sabinbasnet/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/sabinbasnet/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/sabinbasnet/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/sabinbasnet/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sabinbasnet/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#quick way to download specific resources directly from the console is to pass a list to nltk.download()\n",
    "\n",
    "nltk.download([\n",
    "    \"names\",\n",
    "    \"stopwords\",\n",
    "    \"state_union\",\n",
    "    \"twitter_samples\",\n",
    "    \"movie_reviews\",\n",
    "    \"averaged_perceptron_tagger\",\n",
    "    \"vader_lexicon\",\n",
    "    \"punkt\",\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d39459f-2111-4d9a-a417-c95399bddac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = nltk.corpus.shakespeare.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca059096-97a7-42e8-a8d7-a7c5430c3dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling Data\n",
    "words = [w for w in nltk.corpus.state_union.words() if w.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ce90902e-3bd8-4c98-9cb5-7191c66a90d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "64279cf3-02d1-487f-9177-89fca5c54c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stop words from your original word list\n",
    "words = [w for w in words if w.lower() not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c22c3d7e-f266-4517-a189-35097e0f1afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['For', 'some', 'quick', 'analysis', ',', 'creating', 'a', 'corpus', 'could',\n",
      " 'be', 'overkill', '.', 'If', 'all', 'you', 'need', 'is', 'a', 'word', 'list',\n",
      " ',', 'there', 'are', 'simpler', 'ways', 'to', 'achieve', 'that', 'goal', '.']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "text = \"\"\"\n",
    "For some quick analysis, creating a corpus could be overkill. \n",
    "If all you need is a word list, \n",
    "there are simpler ways to achieve that goal.\"\"\"\n",
    "pprint(nltk.word_tokenize(text), width=79, compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "46177dd5-7cc6-486d-8d76-e49be33739f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Frequency Distributions\n",
    "words: list[str] = nltk.word_tokenize(text)\n",
    "fd = nltk.FreqDist(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "46e087ac-533a-493a-9ea9-faec2f04112c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 2), ('a', 2), ('.', 2)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd.most_common(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cb70286e-47d3-47e7-a8a2-a81c3805a2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", a . \n",
      "2 2 2 \n"
     ]
    }
   ],
   "source": [
    "fd.tabulate(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fab93b2f-116a-45a1-a221-db661e33e234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd[\"America\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e091b673-3648-4176-8378-019bd3894f78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd[\"america\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "abcef462-2616-400c-bd04-ed78b09989d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd[\"AMERICA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "854139a2-5f75-4a3c-b36c-4e83cc2902d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try creating a new frequency distribution that’s based on the initial one but normalizes all words to lowercase\n",
    "lower_fd = nltk.FreqDist([w.lower() for w in fd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "61bc5ba0-d118-48d6-9cd0-f7c52a2e1aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 5 of 1079 matches:\n",
      " would want us to do . That is what America will do . So much blood has already\n",
      "ay , the entire world is looking to America for enlightened leadership to peace\n",
      "beyond any shadow of a doubt , that America will continue the fight for freedom\n",
      " to make complete victory certain , America will never become a party to any pl\n",
      "nly in law and in justice . Here in America , we have labored long and hard to \n"
     ]
    }
   ],
   "source": [
    "#Extracting Concordance and Collocations\n",
    "text = nltk.Text(nltk.corpus.state_union.words())\n",
    "text.concordance(\"america\", lines=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "328a0001-608f-4d73-abc9-6e0611fde97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " would want us to do . That is what America will do . So much blood has already\n",
      "ay , the entire world is looking to America for enlightened leadership to peace\n"
     ]
    }
   ],
   "source": [
    "\"\"\"since .concordance() only prints information to the console, it’s \n",
    "not ideal for data manipulation. To obtain a usable list that will also give you\n",
    "information about the location of each occurrence, use .concordance_list()\"\"\"\n",
    "concordance_list = text.concordance_list(\"america\", lines=2)\n",
    "for entry in concordance_list:\n",
    "    print(entry.line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "64ef44cb-ac6f-4af8-bc5d-e8a7bc794d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    is better   than \n",
      "     3      3      3 \n"
     ]
    }
   ],
   "source": [
    "\"\"\"Revisiting nltk.word_tokenize(), check out how quickly you can create a c\n",
    "ustom nltk.Text instance and an accompanying frequency distribution\"\"\"\n",
    "words: list[str] = nltk.word_tokenize(\n",
    "     \"\"\"Beautiful is better than ugly.\n",
    "     Explicit is better than implicit.\n",
    "     Simple is better than complex.\"\"\"\n",
    ")\n",
    "text = nltk.Text(words)\n",
    "fd = text.vocab()  # Equivalent to fd = nltk.FreqDist(words)\n",
    "fd.tabulate(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2115d363-8451-4347-8889-1477909092ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLTK provides specific classes to find collocations text. Following the pattern seen so far, these classes are also built from lists of words\n",
    "words = [w for w in nltk.corpus.state_union.words() if w.isalpha()]\n",
    "finder = nltk.collocations.TrigramCollocationFinder.from_words(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f4d6b64c-1841-4034-be52-3a6a378cfe3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('the', 'United', 'States'), 294), (('the', 'American', 'people'), 185)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using ngram_fd, you can find the most common collocations in the supplied text\n",
    "finder.ngram_fd.most_common(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2ce26889-0c64-4184-91e3-6c4d2e80bca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ('the', 'United', 'States') ('the', 'American', 'people') \n",
      "                          294                           185 \n"
     ]
    }
   ],
   "source": [
    "finder.ngram_fd.tabulate(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "21282795-0aae-435a-b261-225936198447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.295, 'pos': 0.705, 'compound': 0.8012}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using NLTK’s Pre-Trained Sentiment Analyzer\n",
    "#To use VADER, first create an instance of nltk.sentiment.SentimentIntensityAnalyzer, then use .polarity_scores() on a raw string\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores(\"Wow, NLTK is really powerful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "086796a4-ca48-4469-9e90-07b542e33ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the twitter_samples corpus into a list of strings, making a replacement to render URLs inactive to avoid accidental clicks\n",
    "tweets = [t.replace(\"://\", \"//\") for t in nltk.corpus.twitter_samples.strings()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4a611fa8-6ac8-4628-aba1-ee3e241fbfd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> False RT @mygibbo: I hate Tories. And yes, it's tribal | Gary Younge http//t.co/eaqyVXIqCy\n",
      "> True @Awaishooo Shahid Afridi tou young hi hai na abhi. talk about Mahnor baloch. she will never going to be old. :)\n",
      "> False Y is no one up :-(\n",
      "> False i miss them :(\n",
      "> False @KEEMSTARx And people ask why I don't leave my house, the world is scary and fucked. \n",
      "Shootings and explosions everywhere :(\n",
      "> False what usually happens :( https//t.co/6o3ZgNOnvh\n",
      "> True @SpongeZim .. Thank you Mario... Have a wonderful Friday.... :))\n",
      "> True RT @NicolaSturgeon: If Miliband is going to let Tories in rather than work with SNP, we will definitely need lots of SNP MPs to protect Sco…\n",
      "> False RT @billbanjos: I'm #SNPbecause we have witnessed tonight on tv Ed Miliband finally betraying the legacy of James Keir Hardie - #scottishla…\n",
      "> True RT @beaubeau888: Ed Milliband came on top on the Sun's Twitter worm. Bet they won't mention that in their paper tomorrow. http//t.co/QdBJr…\n"
     ]
    }
   ],
   "source": [
    "#use the .polarity_scores() function of your SentimentIntensityAnalyzer instance to classify tweets\n",
    "from random import shuffle\n",
    "\n",
    "def is_positive(tweet: str) -> bool:\n",
    "    \"\"\"True if tweet has positive compound sentiment, False otherwise.\"\"\"\n",
    "    return sia.polarity_scores(tweet)[\"compound\"] > 0\n",
    "\n",
    "shuffle(tweets)\n",
    "for tweet in tweets[:10]:\n",
    "    print(\">\", is_positive(tweet), tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3c4668e7-026b-47d7-98d3-476b48aaa963",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a list of the file IDs that the corpus uses, which you can use later to reference individual reviews\n",
    "positive_review_ids = nltk.corpus.movie_reviews.fileids(categories=[\"pos\"])\n",
    "negative_review_ids = nltk.corpus.movie_reviews.fileids(categories=[\"neg\"])\n",
    "all_review_ids = positive_review_ids + negative_review_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "29bf6813-b9c4-4af8-96ce-6e83e0d6d9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#redefine is_positive() to work on an entire review. we need to obtain that specific review using its file ID \n",
    "#and then split it into sentences before rating\n",
    "from statistics import mean\n",
    "\n",
    "def is_positive(review_id: str) -> bool:\n",
    "    \"\"\"True if the average of all sentence compound scores is positive.\"\"\"\n",
    "    text = nltk.corpus.movie_reviews.raw(review_id)\n",
    "    scores = [\n",
    "        sia.polarity_scores(sentence)[\"compound\"]\n",
    "        for sentence in nltk.sent_tokenize(text)\n",
    "    ]\n",
    "    return mean(scores) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "130a1eba-1fa5-4ae8-80f2-6c8201787907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.00% correct\n"
     ]
    }
   ],
   "source": [
    "# taking the opportunity to rate all the reviews and see how accurate VADER is with this setup\n",
    "shuffle(all_review_ids)\n",
    "correct = 0\n",
    "for review_id in all_review_ids:\n",
    "     if is_positive(review_id):\n",
    "         if review_id in positive_review_ids:\n",
    "             correct += 1\n",
    "     else:\n",
    "         if review_id in negative_review_ids:\n",
    "             correct += 1\n",
    "\n",
    "print(F\"{correct / len(all_review_ids):.2%} correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "51018c68-33f6-464b-8450-ee8cbc5eecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Customizing NLTK’s Sentiment Analysis\n",
    "unwanted = nltk.corpus.stopwords.words(\"english\")\n",
    "unwanted.extend([w.lower() for w in nltk.corpus.names.words()])\n",
    "\n",
    "def skip_unwanted(pos_tuple):\n",
    "    word, tag = pos_tuple\n",
    "    if not word.isalpha() or word in unwanted:\n",
    "        return False\n",
    "    if tag.startswith(\"NN\"):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "positive_words = [word for word, tag in filter(\n",
    "    skip_unwanted,\n",
    "    nltk.pos_tag(nltk.corpus.movie_reviews.words(categories=[\"pos\"]))\n",
    ")]\n",
    "negative_words = [word for word, tag in filter(\n",
    "    skip_unwanted,\n",
    "    nltk.pos_tag(nltk.corpus.movie_reviews.words(categories=[\"neg\"]))\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "34f7202e-8daa-493a-8c32-455fd77a20a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since many words are present in both positive and negative sets, \n",
    "#begin by finding the common set so you can remove it from the distribution objects\n",
    "positive_fd = nltk.FreqDist(positive_words)\n",
    "negative_fd = nltk.FreqDist(negative_words)\n",
    "\n",
    "common_set = set(positive_fd).intersection(negative_fd)\n",
    "\n",
    "for word in common_set:\n",
    "    del positive_fd[word]\n",
    "    del negative_fd[word]\n",
    "\n",
    "top_100_positive = {word for word, count in positive_fd.most_common(100)}\n",
    "top_100_negative = {word for word, count in negative_fd.most_common(100)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "afd0a085-2808-499f-8604-a8f2c2ebcb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up the positive and negative bigram finders\n",
    "unwanted = nltk.corpus.stopwords.words(\"english\")\n",
    "unwanted.extend([w.lower() for w in nltk.corpus.names.words()])\n",
    "\n",
    "positive_bigram_finder = nltk.collocations.BigramCollocationFinder.from_words([\n",
    "    w for w in nltk.corpus.movie_reviews.words(categories=[\"pos\"])\n",
    "    if w.isalpha() and w not in unwanted\n",
    "])\n",
    "negative_bigram_finder = nltk.collocations.BigramCollocationFinder.from_words([\n",
    "    w for w in nltk.corpus.movie_reviews.words(categories=[\"neg\"])\n",
    "    if w.isalpha() and w not in unwanted\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "87c411b4-11e0-4f2f-ab48-9425cb97ed91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training and Using a Classifier\n",
    "#for positive movie reviews, focus on the features that indicate positivity, including VADER scores\n",
    "def extract_features(text):\n",
    "    features = dict()\n",
    "    wordcount = 0\n",
    "    compound_scores = list()\n",
    "    positive_scores = list()\n",
    "\n",
    "    for sentence in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sentence):\n",
    "            if word.lower() in top_100_positive:\n",
    "                wordcount += 1\n",
    "        compound_scores.append(sia.polarity_scores(sentence)[\"compound\"])\n",
    "        positive_scores.append(sia.polarity_scores(sentence)[\"pos\"])\n",
    "\n",
    "    # Adding 1 to the final compound score to always have positive numbers\n",
    "    # since some classifiers you'll use later don't work with negative numbers.\n",
    "    features[\"mean_compound\"] = mean(compound_scores) + 1\n",
    "    features[\"mean_positive\"] = mean(positive_scores)\n",
    "    features[\"wordcount\"] = wordcount\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "186dc989-d88a-4e0e-a55f-4ecdff7eca6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In order to train and evaluate a classifier, we’ll need to build a list of features for each text you’ll analyze\n",
    "features = [\n",
    "    (extract_features(nltk.corpus.movie_reviews.raw(review)), \"pos\")\n",
    "    for review in nltk.corpus.movie_reviews.fileids(categories=[\"pos\"])\n",
    "]\n",
    "features.extend([\n",
    "    (extract_features(nltk.corpus.movie_reviews.raw(review)), \"neg\")\n",
    "    for review in nltk.corpus.movie_reviews.fileids(categories=[\"neg\"])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "53e04dca-505c-4b54-886d-cc75185e6bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "               wordcount = 2                 pos : neg    =      3.2 : 1.0\n",
      "               wordcount = 4                 pos : neg    =      3.0 : 1.0\n",
      "               wordcount = 0                 neg : pos    =      1.6 : 1.0\n",
      "               wordcount = 1                 pos : neg    =      1.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Training the classifier involves splitting the feature set so that one portion can be\n",
    "used for training and the other for evaluation, then calling .train()\"\"\"\n",
    "# Use 1/4 of the set for training\n",
    "train_count = len(features) // 4\n",
    "shuffle(features)\n",
    "classifier = nltk.NaiveBayesClassifier.train(features[:train_count])\n",
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0af93acc-29b8-4eac-a59d-5748f7e4a5bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6633333333333333"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier, features[train_count:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a25173-650f-482f-8757-a694ee9af548",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"To classify new data, find a movie review somewhere and pass it to\n",
    "classifier.classify(). we can also use extract_features() to tell us\n",
    "exactly how it was scored\"\"\"\n",
    "new_review = ...\n",
    "classifier.classify(new_review)\n",
    "extract_features(new_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "899a82f2-4299-4760-864c-545cf79cdaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing Additional Classifiers\n",
    "#The following classifiers are a subset of all classifiers available to you. These will work within NLTK for sentiment analysis\n",
    "from sklearn.naive_bayes import (\n",
    "    BernoulliNB,\n",
    "    ComplementNB,\n",
    "    MultinomialNB,\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "af4f90ac-7539-4a9d-86fb-fe8e444e3be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To aid in accuracy evaluation, it’s helpful to have a mapping of classifier names and their instances\n",
    "classifiers = {\n",
    "    \"BernoulliNB\": BernoulliNB(),\n",
    "    \"ComplementNB\": ComplementNB(),\n",
    "    \"MultinomialNB\": MultinomialNB(),\n",
    "    \"KNeighborsClassifier\": KNeighborsClassifier(),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(),\n",
    "    \"LogisticRegression\": LogisticRegression(),\n",
    "    \"MLPClassifier\": MLPClassifier(max_iter=1000),\n",
    "    \"AdaBoostClassifier\": AdaBoostClassifier(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8780ec9e-c2d5-4fa0-8cfb-1bab1712af1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using scikit-learn Classifiers With NLTK\n",
    "features = [\n",
    "    (extract_features(nltk.corpus.movie_reviews.raw(review)), \"pos\")\n",
    "    for review in nltk.corpus.movie_reviews.fileids(categories=[\"pos\"])\n",
    "]\n",
    "features.extend([\n",
    "    (extract_features(nltk.corpus.movie_reviews.raw(review)), \"neg\")\n",
    "    for review in nltk.corpus.movie_reviews.fileids(categories=[\"neg\"])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3e2653ce-dd1e-400b-954f-814e11d4197f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.27% - BernoulliNB\n",
      "65.73% - ComplementNB\n",
      "65.00% - MultinomialNB\n",
      "68.87% - KNeighborsClassifier\n",
      "62.73% - DecisionTreeClassifier\n",
      "68.67% - RandomForestClassifier\n",
      "71.80% - LogisticRegression\n",
      "72.47% - MLPClassifier\n",
      "69.53% - AdaBoostClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# The first half of the list contains only positive reviews, begin by shuffling it, \n",
    "#then iterate over all classifiers to train and evaluate each one\n",
    "# Use 1/4 of the set for training\n",
    "train_count = len(features) // 4\n",
    "shuffle(features)\n",
    "for name, sklearn_classifier in classifiers.items():\n",
    "     classifier = nltk.classify.SklearnClassifier(sklearn_classifier)\n",
    "     classifier.train(features[:train_count])\n",
    "     accuracy = nltk.classify.accuracy(classifier, features[train_count:])\n",
    "     print(F\"{accuracy:.2%} - {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e9328d-16c2-4b96-afa2-bb3fbb3ec8d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
